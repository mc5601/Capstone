{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79532719-d392-4b88-b2e0-e557c62f3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/jovyan/Capstone/Air_Q\n",
      "Here: ['Untitled.ipynb', '.ipynb_checkpoints', 'graphs_multi', 'relative_plots', 'Zone_1_11am_pm25_vs_dist.png', 'Zone_1_2pm_pm25_vs_dist.png', 'Zone_1_7pm_pm25_vs_dist.png', 'Zone_2_11am_pm25_vs_dist.png', 'zones_by_hour_summary.csv', 'CO2_vs_time.png', 'PM25_vs_time.png', 'AQ_298', 'AQ_389', 'relative_plots_dayviews', 'day_views', 'AQ_205', 'Untitled1.ipynb']\n",
      "Exists? False\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Here:\", [p.name for p in Path('.').iterdir()])\n",
    "print(\"Exists?\", Path(\"service_account.json\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c2235bf-6c1b-48a0-8e7f-264e08d12b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, subprocess\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pydrive2\", \"google-auth-oauthlib\", \"--quiet\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bd0a297-2b56-4a3b-8c37-0d70136063ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD: /home/jovyan/Capstone/Air_Q\n",
      "Here: ['Untitled.ipynb', '.ipynb_checkpoints', 'graphs_multi', 'relative_plots', 'Zone_1_11am_pm25_vs_dist.png', 'Zone_1_2pm_pm25_vs_dist.png', 'Zone_1_7pm_pm25_vs_dist.png', 'Zone_2_11am_pm25_vs_dist.png', 'zones_by_hour_summary.csv', 'CO2_vs_time.png', 'PM25_vs_time.png', 'AQ_298', 'AQ_389', 'relative_plots_dayviews', 'day_views', 'AQ_205', 'Untitled1.ipynb']\n",
      "Exists? False\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Service account JSON not found: service_account.json\nSet env var CAPSTONE_SA_JSON with the full path OR place service_account.json next to this script.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 66\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(out)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# --- Login + download to cache ---\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m _drive \u001b[38;5;241m=\u001b[39m \u001b[43m_login_sa\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSA_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m _txt_files_api \u001b[38;5;241m=\u001b[39m _download_logs(_drive, FOLDER_ID, LOCAL_CACHE)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _txt_files_api:\n",
      "Cell \u001b[0;32mIn[13], line 30\u001b[0m, in \u001b[0;36m_login_sa\u001b[0;34m(sa_json_path)\u001b[0m\n\u001b[1;32m     28\u001b[0m p \u001b[38;5;241m=\u001b[39m Path(sa_json_path)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mService account JSON not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSet env var CAPSTONE_SA_JSON with the full path OR place service_account.json next to this script.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m gauth \u001b[38;5;241m=\u001b[39m GoogleAuth()\n\u001b[1;32m     35\u001b[0m gauth\u001b[38;5;241m.\u001b[39mServiceAccountCredentials(\u001b[38;5;28mstr\u001b[39m(p), scope\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.googleapis.com/auth/drive.readonly\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Service account JSON not found: service_account.json\nSet env var CAPSTONE_SA_JSON with the full path OR place service_account.json next to this script."
     ]
    }
   ],
   "source": [
    "# ==== Google Drive (Service Account) — Single Source of Truth ====\n",
    "# Download all PSP*_LOG_*UTC*.txt from your Capstone Drive folder (by FOLDER_ID)\n",
    "# into a local cache, then run the pipeline on that cache.\n",
    "from pathlib import Path\n",
    "print(\"CWD:\", Path.cwd())\n",
    "print(\"Here:\", [p.name for p in Path('.').iterdir()])\n",
    "print(\"Exists?\", Path(\"service_account.json\").exists())\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import os, sys, subprocess\n",
    "\n",
    "# --- CONFIG (edit/override as needed) ---\n",
    "FOLDER_ID = os.getenv(\"CAPSTONE_FOLDER_ID\", \"1YNujfIm14j_lMgDodoV8cvQksWZu_gdy\")  # your Air_Quality folder ID\n",
    "SA_FILE   = os.getenv(\"CAPSTONE_SA_JSON\", \"service_account.json\")                  # path to SA JSON\n",
    "LOCAL_CACHE = Path(\"./_gdrive_cache\")                                             # local cache for logs\n",
    "OUTPUT_DIR  = LOCAL_CACHE / \"outputs\"                                             # where PNG/CSV will be saved\n",
    "\n",
    "# --- Dependencies ---\n",
    "try:\n",
    "    import pydrive2  # noqa: F401\n",
    "except ModuleNotFoundError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pydrive2\", \"--quiet\"])\n",
    "\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "\n",
    "def _login_sa(sa_json_path: str) -> GoogleDrive:\n",
    "    p = Path(sa_json_path)\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"Service account JSON not found: {p}\\n\"\n",
    "            \"Set env var CAPSTONE_SA_JSON with the full path OR place service_account.json next to this script.\"\n",
    "        )\n",
    "    gauth = GoogleAuth()\n",
    "    gauth.ServiceAccountCredentials(str(p), scope=[\"https://www.googleapis.com/auth/drive.readonly\"])\n",
    "    return GoogleDrive(gauth)\n",
    "\n",
    "def _walk_files(drive, root_id: str):\n",
    "    FOLDER = \"application/vnd.google-apps.folder\"\n",
    "    stack = [root_id]\n",
    "    while stack:\n",
    "        pid = stack.pop()\n",
    "        # folders\n",
    "        for fld in drive.ListFile({'q': f\"'{pid}' in parents and mimeType='{FOLDER}' and trashed=false\"}).GetList():\n",
    "            stack.append(fld['id'])\n",
    "        # files\n",
    "        for f in drive.ListFile({'q': f\"'{pid}' in parents and mimeType!='{FOLDER}' and trashed=false\"}).GetList():\n",
    "            yield f\n",
    "\n",
    "def _download_logs(drive, folder_id: str, outdir: Path) -> List[Path]:\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    out: List[Path] = []\n",
    "    for meta in _walk_files(drive, folder_id):\n",
    "        name = meta.get(\"name\") or meta.get(\"title\")\n",
    "        if not name:\n",
    "            continue\n",
    "        up = name.upper()\n",
    "        if up.endswith(\".TXT\") and (\"LOG\" in up) and (\"PSP\" in up) and (\"UTC\" in up):\n",
    "            p = outdir / name\n",
    "            if not p.exists():\n",
    "                drive.CreateFile({'id': meta['id']}).GetContentFile(str(p))\n",
    "            out.append(p)\n",
    "    return sorted(out)\n",
    "\n",
    "# --- Login + download to cache ---\n",
    "_drive = _login_sa(SA_FILE)\n",
    "_txt_files_api = _download_logs(_drive, FOLDER_ID, LOCAL_CACHE)\n",
    "if not _txt_files_api:\n",
    "    raise SystemExit(\"No TXT logs found in the Drive folder (by ID).\")\n",
    "\n",
    "# ROOT stays as the local cache; don't overwrite it later\n",
    "ROOT = LOCAL_CACHE.resolve()\n",
    "\n",
    "# Make the loader return the downloaded list (do NOT redefine later)\n",
    "def list_upas_txts(root: Path) -> List[Path]:\n",
    "    return _txt_files_api\n",
    "# ==== End Drive header ====\n",
    "\n",
    "\n",
    "# ---------------------- Your original pipeline (adapted) ----------------------\n",
    "# (Key change: we DO NOT reassign ROOT here; we use the one from the header.)\n",
    "from io import StringIO\n",
    "import re\n",
    "from typing import Optional, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "SAVE_PNG = True\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PNG_DIR = OUTPUT_DIR / \"graphs_multi\"\n",
    "PNG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_CSV = True\n",
    "CSV_PATH = OUTPUT_DIR / \"zones_by_hour_summary.csv\"\n",
    "\n",
    "# ---------------------- Name patterns ----------------------\n",
    "FNAME_RE = re.compile(\n",
    "    r\"^(?P<prefix>PSP\\d+)_LOG_(?P<stamp>\\d{4}-\\d{2}-\\d{2}T\\d{2}_\\d{2}_\\d{2})UTC.*\\.txt$\",\n",
    "    re.IGNORECASE,\n",
    ")\n",
    "DEVICE_FOLDER_RE = re.compile(r\".*\\bAQ_(\\d+)\\b\", re.IGNORECASE)\n",
    "PSP_IN_NAME_RE = re.compile(r\"\\b(PSP\\d{5})\\b\", re.IGNORECASE)\n",
    "\n",
    "# ---------------------- Utilities ----------------------\n",
    "def safe_relpath(p: Path, root: Path) -> str:\n",
    "    try:\n",
    "        return str(p.resolve().relative_to(root.resolve()))\n",
    "    except Exception:\n",
    "        try:\n",
    "            return os.path.relpath(p.resolve(), start=root.resolve())\n",
    "        except Exception:\n",
    "            return p.name\n",
    "\n",
    "def read_upas_sample_log(path: Path) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    text = path.read_text(encoding=\"utf-8\", errors=\"ignore\").splitlines()\n",
    "    try:\n",
    "        i = text.index(\"SAMPLE LOG\")\n",
    "    except ValueError:\n",
    "        raise RuntimeError(f\"'SAMPLE LOG' not found in {path.name}\")\n",
    "    header_idx = None\n",
    "    for j in range(i + 1, min(i + 25, len(text))):\n",
    "        if \"DateTimeUTC\" in text[j]:\n",
    "            header_idx = j\n",
    "            break\n",
    "    if header_idx is None:\n",
    "        raise RuntimeError(f\"Could not find column headers in {path.name}\")\n",
    "    header = text[header_idx]\n",
    "    units_idx = header_idx + 1\n",
    "    data_start = units_idx + 1\n",
    "    csv_buf = header + \"\\n\" + \"\\n\".join(text[data_start:])\n",
    "    df = pd.read_csv(StringIO(csv_buf))\n",
    "    df = df.replace([-9999.0, -9999], np.nan)\n",
    "    return df, text\n",
    "\n",
    "def device_from_path_or_file(path: Path, lines: List[str]) -> str:\n",
    "    m = DEVICE_FOLDER_RE.search(str(path.parent))\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    m = PSP_IN_NAME_RE.search(path.name)\n",
    "    if m:\n",
    "        return m.group(1).upper()\n",
    "    for line in lines[:200]:\n",
    "        if line.startswith(\"UPASserial\"):\n",
    "            parts = line.split(\",\")\n",
    "            if len(parts) > 1:\n",
    "                digits = \"\".join(ch for ch in parts[1] if ch.isdigit())\n",
    "                return digits or parts[1].strip()\n",
    "            break\n",
    "    return path.stem.split(\"_\")[0]\n",
    "\n",
    "def parse_absolute_time(df: pd.DataFrame) -> pd.Series:\n",
    "    utc_col = next((c for c in df.columns if \"DateTimeUTC\" in c), None)\n",
    "    if utc_col:\n",
    "        ts = pd.to_datetime(df[utc_col], format=\"%Y-%m-%dT%H:%M:%S\", errors=\"coerce\", utc=True)\n",
    "        if ts.notna().any():\n",
    "            return ts\n",
    "    if \"UnixTime\" in df.columns:\n",
    "        ts = pd.to_datetime(pd.to_numeric(df[\"UnixTime\"], errors=\"coerce\"), unit=\"s\", utc=True)\n",
    "        if ts.notna().any():\n",
    "            return ts\n",
    "    loc_col = next((c for c in df.columns if \"DateTimeLocal\" in c), None)\n",
    "    if loc_col:\n",
    "        return pd.to_datetime(df[loc_col], errors=\"coerce\")\n",
    "    return pd.Series(pd.RangeIndex(len(df)), index=df.index, dtype=\"int64\")\n",
    "\n",
    "def start_local_time(lines: List[str], df: pd.DataFrame, path: Path) -> Optional[pd.Timestamp]:\n",
    "    try:\n",
    "        j = lines.index(\"SAMPLE SUMMARY\")\n",
    "        for k in range(j, min(j + 80, len(lines))):\n",
    "            if lines[k].startswith(\"StartDateTimeLocal,\"):\n",
    "                val = lines[k].split(\",\")[1].strip()\n",
    "                dt = pd.to_datetime(val, errors=\"coerce\")\n",
    "                if pd.notna(dt):\n",
    "                    return dt\n",
    "                break\n",
    "    except ValueError:\n",
    "        pass\n",
    "    loc_col = next((c for c in df.columns if \"DateTimeLocal\" in c), None)\n",
    "    if loc_col:\n",
    "        dt = pd.to_datetime(df[loc_col], errors=\"coerce\")\n",
    "        if dt.notna().any():\n",
    "            return dt.dropna().iloc[0]\n",
    "    m = FNAME_RE.match(path.name)\n",
    "    if m:\n",
    "        ts = pd.to_datetime(m.group(\"stamp\").replace(\"_\", \":\"), errors=\"coerce\")\n",
    "        if pd.notna(ts):\n",
    "            return ts\n",
    "    return None\n",
    "\n",
    "def hour_bucket_label(dt: Optional[pd.Timestamp]) -> str:\n",
    "    if dt is None or pd.isna(dt):\n",
    "        return \"?\"\n",
    "    return dt.round(\"h\").strftime(\"%I %p\").lstrip(\"0\").upper()\n",
    "\n",
    "def date_label(dt: Optional[pd.Timestamp]) -> str:\n",
    "    if dt is None or pd.isna(dt):\n",
    "        return \"unknown-date\"\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ---------------------- Load & prepare ----------------------\n",
    "txt_files = list_upas_txts(ROOT)\n",
    "print(f\"Found {len(txt_files)} TXT files under {ROOT.resolve()}.\")\n",
    "\n",
    "rows = []\n",
    "summary_rows = []\n",
    "\n",
    "for path in txt_files:\n",
    "    try:\n",
    "        df, lines = read_upas_sample_log(path)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping {safe_relpath(path, ROOT)}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if \"PM2_5MC\" not in df.columns:\n",
    "        print(f\"Skipping {safe_relpath(path, ROOT)}: PM2_5MC missing.\")\n",
    "        continue\n",
    "\n",
    "    ts_abs = parse_absolute_time(df)\n",
    "    pm = pd.to_numeric(df[\"PM2_5MC\"], errors=\"coerce\")\n",
    "\n",
    "    if ts_abs.notna().any():\n",
    "        t0 = ts_abs.dropna().iloc[0]\n",
    "    else:\n",
    "        print(f\"Skipping {safe_relpath(path, ROOT)}: no valid timestamps.\")\n",
    "        continue\n",
    "\n",
    "    t_rel = (ts_abs - t0)\n",
    "    t_rel_min = t_rel.dt.total_seconds() / 60.0 if hasattr(t_rel, \"dt\") else pd.Series(np.nan, index=df.index)\n",
    "\n",
    "    dev = device_from_path_or_file(path, lines)\n",
    "    start_loc = start_local_time(lines, df, path)\n",
    "    dlabel = date_label(start_loc)\n",
    "    hlabel = hour_bucket_label(start_loc)\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"file\": safe_relpath(path, ROOT),\n",
    "        \"device\": dev,\n",
    "        \"date\": dlabel,\n",
    "        \"hour_bucket\": hlabel,\n",
    "    })\n",
    "\n",
    "    ok = pd.Series(t_rel_min).notna() & pm.notna()\n",
    "    if ok.any():\n",
    "        rows.append(pd.DataFrame({\n",
    "            \"file\": safe_relpath(path, ROOT),\n",
    "            \"device\": dev,\n",
    "            \"date\": dlabel,\n",
    "            \"hour_bucket\": hlabel,\n",
    "            \"t_rel_min\": t_rel_min[ok].to_numpy(),\n",
    "            \"PM2_5MC\": pm[ok].to_numpy(),\n",
    "        }))\n",
    "\n",
    "if not rows:\n",
    "    raise SystemExit(\"No valid PM2_5MC + time series found.\")\n",
    "\n",
    "data = pd.concat(rows, ignore_index=True)\n",
    "summary = pd.DataFrame(summary_rows).drop_duplicates().sort_values([\"date\", \"hour_bucket\", \"device\"])\n",
    "\n",
    "# ---------------------- Plot by (date, hour) ----------------------\n",
    "if SAVE_PNG:\n",
    "    for (date_k, hour_k), g in data.groupby([\"date\", \"hour_bucket\"], sort=True):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for fname, gg in g.groupby(\"file\"):\n",
    "            dev = gg[\"device\"].iloc[0]\n",
    "            x = gg[\"t_rel_min\"].to_numpy()\n",
    "            y = gg[\"PM2_5MC\"].to_numpy()\n",
    "            if len(x) == 0:\n",
    "                continue\n",
    "            order = np.argsort(x)\n",
    "            x, y = x[order], y[order]\n",
    "            label = f\"{dev} · start ~{hour_k}\"\n",
    "            plt.plot(x, y, label=label, linewidth=1.6)\n",
    "            plt.plot(x[0], y[0], marker=\"*\", markersize=9)\n",
    "        plt.xlabel(\"Relative time from start (min)\")\n",
    "        plt.ylabel(\"PM2.5 (µg/m³)\")\n",
    "        plt.title(f\"PM2.5 vs time (aligned) · {date_k} · ~{hour_k}\")\n",
    "        plt.legend(loc=\"best\", fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        outfile = PNG_DIR / f\"{date_k}_{hour_k.replace(' ', '')}_pm25_vs_time.png\"\n",
    "        plt.savefig(outfile, dpi=150)\n",
    "        plt.show()\n",
    "\n",
    "# ---------------------- Save summary ----------------------\n",
    "if SAVE_CSV:\n",
    "    summary.to_csv(CSV_PATH, index=False)\n",
    "    print(f\"\\nSummary saved to: {CSV_PATH}\")\n",
    "\n",
    "print(\"\\nSummary:\")\n",
    "print(summary.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf5a06-79a1-457d-ab2f-2d2576efab9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
